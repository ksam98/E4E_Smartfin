{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a PSD plot framework for future plots (this one first uses double integration to convert raw acceleration to raw displacement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Purisa Jasmine Simmons\n",
    "#August 2018\n",
    "\n",
    "#Overview: PSD for E3 (Buoy_Calibrator1), trying to generate a PSD plot (framework for future plots).\n",
    "#Based on Method V of this paper: https://journals.ametsoc.org/doi/pdf/10.1175/2010JTECHO724.1\n",
    "\n",
    "#First, parse the data from the .CSV file.\n",
    "#This data comes from a controlled experiment (CE3), so we are assuming that \n",
    "#all of the vertical accelerations are contained in IMUA2.\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from scipy import constants\n",
    "from scipy import signal #added\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import simps\n",
    "from scipy.integrate import cumtrapz\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "\n",
    "import peakutils\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import requests\n",
    "\n",
    "import mpld3\n",
    "import folium\n",
    "#import cmocean\n",
    "import skinematics as skin\n",
    "from skinematics import quat, vector, misc, rotmat, imus, view\n",
    "import pygame\n",
    "\n",
    "from plotly import tools #added all the plotly's\n",
    "import plotly.offline\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import math #added\n",
    "import re   #added\n",
    "\n",
    "# For the definition of the abstract base class IMU_Base\n",
    "import abc\n",
    "\n",
    "import sys\n",
    "\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_ids = ['14827']\n",
    "# 14743 - Motion Control July 10th\n",
    "# 14750 - Magnetometer Control July 11th\n",
    "# 14814 - Pool Displacement Control July 17th\n",
    "# 14815 - Compass Orientation (Lying on Charger Side) July 19th\n",
    "# 14816 - Orientation w Higher Sampling (Lying on Charger Side) July 20th\n",
    "# 14827 - Pool Displacement Control w Higher Sampling (Jul 23)\n",
    "# 14888 - First Buoy Calibration Experiment (July 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Fin ID scraper\n",
    "# Input fin ID, get all ride IDs\n",
    "# base URL to which we'll append given fin IDs\n",
    "fin_url_base = 'http://surf.smartfin.org/fin/'\n",
    "\n",
    "# Look for the following text in the HTML contents in fcn below\n",
    "str_id_ride = 'rideId = \\'' # backslash allows us to look for single quote\n",
    "str_id_date = 'var date = \\'' # backslash allows us to look for single quote\n",
    "\n",
    "#%% Ride ID scraper\n",
    "# Input ride ID, get ocean and motion CSVs\n",
    "# Base URL to which we'll append given ride IDs\n",
    "ride_url_base = 'https://surf.smartfin.org/ride/'\n",
    "\n",
    "# Look for the following text in the HTML contents in fcn below\n",
    "str_id_csv = 'img id=\"temperatureChart\" class=\"chart\" src=\"' \n",
    "\n",
    "def get_csv_from_ride_id(rid):\n",
    "    # Build URL for each individual ride\n",
    "    ride_url = ride_url_base+str(rid)\n",
    "    print(ride_url)\n",
    "    \n",
    "    # Get contents of ride_url\n",
    "    html_contents = requests.get(ride_url).text\n",
    "    \n",
    "    # Find CSV identifier \n",
    "    loc_csv_id = html_contents.find(str_id_csv)\n",
    "    \n",
    "    # Different based on whether user logged in with FB or Google\n",
    "    offset_googleOAuth = [46, 114]\n",
    "    offset_facebkOAuth = [46, 112]\n",
    "    if html_contents[loc_csv_id+59] == 'f': # Facebook login\n",
    "        off0 = offset_facebkOAuth[0]\n",
    "        off1 = offset_facebkOAuth[1]\n",
    "    else: # Google login\n",
    "        off0 = offset_googleOAuth[0]\n",
    "        off1 = offset_googleOAuth[1]\n",
    "        \n",
    "    csv_id_longstr = html_contents[loc_csv_id+off0:loc_csv_id+off1]\n",
    "    \n",
    "#    print(csv_id_longstr)\n",
    "    \n",
    "    # Stitch together full URL for CSV\n",
    "    if (\"media\" in csv_id_longstr) & (\"Calibration\" not in html_contents): # other junk URLs can exist and break everything\n",
    "        \n",
    "        ocean_csv_url = 'https://surf.smartfin.org/'+csv_id_longstr+'Ocean.CSV'\n",
    "        motion_csv_url = 'https://surf.smartfin.org/'+csv_id_longstr+'Motion.CSV'\n",
    "        \n",
    "        print(ocean_csv_url)\n",
    "        # Go to ocean_csv_url and grab contents (theoretically, a CSV)\n",
    "        ocean_df_small = pd.read_csv(ocean_csv_url, parse_dates = [0])\n",
    "        elapsed_timedelta = (ocean_df_small['UTC']-ocean_df_small['UTC'][0])\n",
    "        ocean_df_small['elapsed'] = elapsed_timedelta/np.timedelta64(1, 's')\n",
    "        \n",
    "        motion_df_small = pd.read_csv(motion_csv_url, parse_dates = [0])\n",
    "        \n",
    "        # Reindex on timestamp if there are at least a few rows\n",
    "        if len(ocean_df_small) > 1:\n",
    "            ocean_df_small.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "            motion_df_small.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "            \n",
    "            print(ocean_df_small)\n",
    "            \n",
    "            \n",
    "            #May need to change this sampling interval:\n",
    "            sample_interval = '33ms'\n",
    "            \n",
    "            \n",
    "            ocean_df_small_resample = ocean_df_small.resample(sample_interval).mean()\n",
    "            motion_df_small_resample = motion_df_small.resample(sample_interval).mean()\n",
    "            \n",
    "            # No need to save many extra rows with no fix\n",
    "            motion_df_small = motion_df_small[~np.isnan(motion_df_small.Latitude)]\n",
    "            \n",
    "            return ocean_df_small_resample, motion_df_small_resample\n",
    "\n",
    "    else:\n",
    "        ocean_df_small_resample = pd.DataFrame() # empty DF just so something is returned\n",
    "        motion_df_small_resample = pd.DataFrame() \n",
    "        return ocean_df_small_resample, motion_df_small_resample\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://surf.smartfin.org/ride/14827\n",
      "https://surf.smartfin.org/media/201807/google_117589279598321562176_000666D321BE_180723164430_Ocean.CSV\n",
      "Ride threw an exception!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-699ecb3b435a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# appended_ocean_df.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mdf_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappended_multiIndex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# keys gotta be a tuple, a list which data in it cannot be changed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mocean_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappended_ocean_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ride_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mmotion_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappended_motion_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ride_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "appended_ocean_list = [] # list of DataFrames from original CSVs\n",
    "appended_motion_list = []\n",
    "appended_multiIndex = [] # fin_id & ride_id used to identify each DataFrame\n",
    "\n",
    "## Nested loops (for each fin ID, find all ride IDs, then build a DataFrame from all ride CSVs)\n",
    "## (Here, ride IDS are either ocean or motion dataframes)\n",
    "count_good_fins = 0\n",
    "    \n",
    "# Loop over ride_ids and find CSVs\n",
    "for rid in ride_ids:\n",
    "    try:\n",
    "        new_ocean_df, new_motion_df = get_csv_from_ride_id(rid) # get given ride's CSV from its ride ID using function above\n",
    "        #print(len(new_ocean_df))\n",
    "        #print(len(new_motion_df))\n",
    "        if not new_ocean_df.empty: # Calibration rides, for example\n",
    "            # Append only if DF isn't empty. There may be a better way to control empty DFs which are created above\n",
    "            appended_multiIndex.append(str(rid)) # build list to be multiIndex of future DataFrame\n",
    "            appended_ocean_list.append(new_ocean_df)\n",
    "            appended_motion_list.append(new_motion_df)\n",
    "            print(\"Ride data has been uploaded.\")\n",
    "            #print(\"Ride: \", rid, \"data has been uploaded.\")\n",
    "            count_good_fins += 1\n",
    "        \n",
    "    except: \n",
    "        print(\"Ride threw an exception!\")\n",
    "        #print(\"Ride \", rid, \"threw an exception!\")    \n",
    "\n",
    "#%% Build the \"Master\" DataFrame\n",
    "\n",
    "# appended_ocean_df.summary()\n",
    "df_keys = tuple(appended_multiIndex) # keys gotta be a tuple, a list which data in it cannot be changed\n",
    "ocean_df = pd.concat(appended_ocean_list, keys = df_keys, names=['ride_id'])\n",
    "motion_df = pd.concat(appended_motion_list, keys = df_keys, names = ['ride_id'])\n",
    "\n",
    "\n",
    "##Here, maybe just use info from the motion_df and don't worry about ocean_df data for now.\n",
    "##If you do want ocean_df data, look at how Phil was getting it from \"July 10th and 11th Calibration\" jupyter notebook file.\n",
    "#print(motion_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of the original motion_df dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(motion_df)\n",
    "\n",
    "saved_copy_motion_df = motion_df.copy(deep=True) #make a copy of the dataframe with raw data\n",
    "\n",
    "print(saved_copy_motion_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcualting time_elapseds, time_offsets and creating IMU1, IMU2, and IMU3 raw data lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data from ride_ids = ['xxxxx']\n",
    "#The name of the motion dataframe is: motion_df\n",
    "\n",
    "#Get times from the \"Time\" column to create time_o_list and time_e_list.\n",
    "#Get imus from the \"IMU A[*]\" column to create the imu acc arrays. \n",
    "\n",
    "#Drop the \"nan\" values from the columns that we care about. \n",
    "dropped_motion_df = motion_df.dropna(subset=['Time', 'IMU A1', 'IMU A2', 'IMU A3'])\n",
    "\n",
    "#Can test that this works by printing this one:\n",
    "#dropped_motion_df = motion_df.dropna(subset=['Time', 'IMU A1', 'IMU A2', 'IMU A3', 'Latitude'])\n",
    "#print(dropped_df)\n",
    "\n",
    "time_e_list = []\n",
    "time_o_list = []\n",
    "\n",
    "#Remove all nan instances in time:\n",
    "time_array_nans = np.array(dropped_motion_df.loc[:,\"Time\"], dtype=float)\n",
    "time_array = []\n",
    "\n",
    "imu1_array_nans = np.array(dropped_motion_df.loc[:,\"IMU A1\"], dtype=float)\n",
    "imu_array1 = []\n",
    "\n",
    "imu2_array_nans = np.array(dropped_motion_df.loc[:,\"IMU A2\"], dtype=float)\n",
    "imu_array2 = []\n",
    "\n",
    "imu3_array_nans = np.array(dropped_motion_df.loc[:,\"IMU A3\"], dtype=float)\n",
    "imu_array3 = []\n",
    "\n",
    "\n",
    "#Get all the times and imus where time, imu1, imu2, and imu3 are NOT nan values:\n",
    "for t,x,y,z in zip(time_array_nans, imu1_array_nans, imu2_array_nans, imu3_array_nans):\n",
    "    if (np.isnan(t)==0 and np.isnan(x)==0 and np.isnan(y)==0 and np.isnan(z)==0):\n",
    "        time_array.append(t)\n",
    "        imu_array1.append(x)\n",
    "        imu_array2.append(y)\n",
    "        imu_array3.append(z)\n",
    "\n",
    "#for x in time_array:\n",
    "#    print(x)\n",
    "    \n",
    "start_time = time_array[0]\n",
    "time_len = len(time_array)\n",
    "    \n",
    "i = 0\n",
    "while (i < time_len - 1):\n",
    "    prev = time_array[i]\n",
    "    after = time_array[i+1]\n",
    "    \n",
    "    #print(prev, \" \", after)\n",
    "    #print(after - prev)\n",
    "    \n",
    "    offset = after - prev\n",
    "    #if (np.isnan(offset)==0):\n",
    "    time_o_list.append(offset)\n",
    "    \n",
    "    elapsed = time_array[i] - start_time\n",
    "    #if (np.isnan(elapsed)==0):\n",
    "    time_e_list.append(elapsed)\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "##Check to make sure there are no \"nan\" values:\n",
    "i = 0\n",
    "while (i < len(time_o_list)):\n",
    "    if (np.isnan(time_o_list[i])):\n",
    "        print(\"Error! Value at index: \", i, \" is nan\")\n",
    "    i = i + 1\n",
    "\n",
    "#Drop the last value from each of the imu lists to make it match the time list.\n",
    "del(imu_array1[-1])\n",
    "del(imu_array2[-1])\n",
    "del(imu_array3[-1])\n",
    "    \n",
    "print(len(time_e_list))\n",
    "print(len(time_o_list))\n",
    "print(len(imu_array1))\n",
    "print(len(imu_array2))\n",
    "print(len(imu_array3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert raw units to actual units (acc to [m/s^2]) and (time to [s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert raw units to actual units (acc to [m/s^2]) and (time to [s])\n",
    "\n",
    "\n",
    "#Raw acceleration constant 512 = 1g (accelerometer's measured force due to gravity)\n",
    "g_const = 512\n",
    "\n",
    "#Approximate measurement for gravity:\n",
    "gravity = -9.80665\n",
    "\n",
    "\n",
    "# Correct the IMU Acceleration columns into units of meters\n",
    "# Dividing by 512 is equivalent to muliplying by 4 to correct the bit shifting by 2 places and dividing by 2048 to convert bits to G's\n",
    "# Multiplying by the 9.81 afterwards is simply to convert G's into m/s^2\n",
    "\n",
    "def convert_acc_units(acc_array):\n",
    "    ret_array = []\n",
    "    for a in acc_array:\n",
    "        #Acceleration is now in m/s^2, need to subtract gravity from vertical axis. (??)\n",
    "        new_a = a / g_const * gravity - gravity\n",
    "        ret_array.append(new_a)\n",
    "    return ret_array\n",
    "\n",
    "imu1_array = convert_acc_units(imu_array1) #new units in m/s^2\n",
    "imu2_array = convert_acc_units(imu_array2) #new units in m/s^2\n",
    "imu3_array = convert_acc_units(imu_array3) #new units in m/s^2\n",
    "\n",
    "##To check:\n",
    "#for x,y in zip(imu2_array, imu_array2):\n",
    "#    print(x,y)\n",
    "    \n",
    "    \n",
    "def convert_time_units(time_array):\n",
    "    ret_array = []\n",
    "    for t in time_array:\n",
    "        new_t = t * (10**(-3)) #converting units in milliseconds to seconds\n",
    "        ret_array.append(new_t)\n",
    "    return ret_array\n",
    "\n",
    "time_o_array = convert_time_units(time_o_list) #new units in seconds\n",
    "time_e_array = convert_time_units(time_e_list) #new units in seconds\n",
    "\n",
    "##To check:\n",
    "#for t in time_e_array:\n",
    "#    print(t)\n",
    "\n",
    "print(len(time_e_array))\n",
    "print(len(time_e_list))\n",
    "print(len(imu2_array))\n",
    "\n",
    "print(\"Graph of our entire experiment:\")\n",
    "\n",
    "plt.plot(time_e_array, imu2_array)\n",
    "plt.show()\n",
    "\n",
    "#print(\"Why are the y-axis values so small?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD Step 1: Seperate each of the subexperiments into its own acc lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in time_e_array:\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate each of the subexperiments into its own acc lists.\n",
    "#i.e. subexperiment1 corresponds to acc1, (subexperiment2 => acc2), etc.\n",
    "\n",
    "time_e_list1 = []\n",
    "time_e_list2 = []\n",
    "time_e_list3 = []\n",
    "\n",
    "acc_list = []\n",
    "acc_list1 = []\n",
    "acc_list2 = []\n",
    "acc_list3 = []\n",
    "\n",
    "time_array = []\n",
    "acc_array = []\n",
    "\n",
    "gravity = -9.80665\n",
    "\n",
    "#For our controlled experiments, we know that imu2 is the vertical axis\n",
    "acc_list = imu2_array\n",
    "\n",
    "\n",
    "#i = 0\n",
    "#while (i < len(imu2_array)):\n",
    "#    imu2_array[i] = imu2_array[i] - gravity\n",
    "#    i = i + 1\n",
    "#print(imu2_array)\n",
    "\n",
    "i = 0\n",
    "while (i < (len(acc_list)) - 1):\n",
    "    if (time_e_array[i] > 300 and time_e_array[i] <= 450):\n",
    "        acc_list1.append(acc_list[i])\n",
    "        time_e_list1.append(time_e_array[i])\n",
    "    if (time_e_array[i] > 450 and time_e_array[i] <= 670):\n",
    "        acc_list2.append(acc_list[i])\n",
    "        time_e_list2.append(time_e_array[i])\n",
    "    if (time_e_array[i] > 670 and time_e_array[i] <= 850):\n",
    "        acc_list3.append(acc_list[i])\n",
    "        time_e_list3.append(time_e_array[i])\n",
    "    i = i + 1\n",
    "    \n",
    "#Plot the subexperiments to verify correctness:\n",
    "for a,t in zip(acc_list,time_e_array): #acc_array becomes only acc values we care about\n",
    "    if t > 300 and t <= 850:\n",
    "        acc_array.append(a)\n",
    "        time_array.append(t)\n",
    "\n",
    "time_array = np.array(time_array)\n",
    "acc_array = np.array(acc_array)\n",
    "time_array1 = np.array(time_e_list1)\n",
    "acc_array1 = np.array(acc_list1)\n",
    "time_array2 = np.array(time_e_list2)\n",
    "acc_array2 = np.array(acc_list2)\n",
    "time_array3 = np.array(time_e_list3)\n",
    "acc_array3 = np.array(acc_list3)\n",
    "  \n",
    "    \n",
    "##Plotting:\n",
    "f1 = plt.figure(figsize=(10,3))\n",
    "ax1 = f1.add_subplot(121)\n",
    "ax2 = f1.add_subplot(122)\n",
    "\n",
    "f2 = plt.figure(figsize=(10,3))\n",
    "ax3 = f2.add_subplot(121)\n",
    "ax4 = f2.add_subplot(122)\n",
    "\n",
    "ax1.plot(time_array, acc_array)\n",
    "ax1.set_title(\"Acceleration vs. Time\")\n",
    "ax1.set_xlabel(\"Time [s]\")\n",
    "ax1.set_ylabel('Acceleration [m/s^2]')\n",
    "ax1.set_xlim([300,850])\n",
    "ax1.set_ylim([-6,6])\n",
    "\n",
    "ax2.plot(time_array1, acc_array1)\n",
    "ax2.set_title(\"Acceleration1 vs. Time\")\n",
    "ax2.set_xlabel(\"Time [s]\")\n",
    "ax2.set_ylabel(\"Acceleration [m/s^2]\")\n",
    "#ax2.set_ylim([-6,6])\n",
    "\n",
    "ax3.plot(time_array2, acc_array2)\n",
    "ax3.set_title(\"Acceleration2 vs. Time\")\n",
    "ax3.set_xlabel(\"Time [s]\")\n",
    "ax3.set_ylabel(\"Acceleration [m/s^2]\")\n",
    "#ax3.set_ylim([-6,6])\n",
    "\n",
    "ax4.plot(time_array3, acc_array3)\n",
    "ax4.set_title(\"Acceleration3 vs. Time\")\n",
    "ax4.set_xlabel(\"Time [s]\")\n",
    "ax4.set_ylabel(\"Acceleration [m/s^2]\")\n",
    "#ax4.set_ylim([-6,6])\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD Step 2: Data Processing and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PSD Step 2: Detrend the data \n",
    "dacc_array1 = signal.detrend(acc_array1)\n",
    "dacc_array2 = signal.detrend(acc_array2)\n",
    "dacc_array3 = signal.detrend(acc_array3)\n",
    "\n",
    "##Remove outliers--points greater than 3x the standard deviation\n",
    "std1 = np.std(dacc_array1)*3\n",
    "std2 = np.std(dacc_array2)*3\n",
    "std3 = np.std(dacc_array3)*3\n",
    "\n",
    "#Returns a new array that is the same as the array passed in, with its outliers removed.\n",
    "def removed_outliers(a_array, time_array, std):\n",
    "    i = 0\n",
    "    count = 0\n",
    "    ret_accs = []\n",
    "    ret_times = []\n",
    "    while i < (len(a_array)):\n",
    "        #if smaller than std, keep that value (larger ones get removed)\n",
    "        if abs(a_array[i]) < std:\n",
    "            ret_accs.append(a_array[i])\n",
    "            ret_times.append(time_array[i])\n",
    "        else:\n",
    "            count = count + 1  #could help with debugging to know how many outliers removed\n",
    "        i = i + 1  \n",
    "    return count, ret_accs, ret_times;\n",
    "        \n",
    "count1, ro_array1, ro_time1 = removed_outliers(dacc_array1, time_array1, std1)\n",
    "count2, ro_array2, ro_time2 = removed_outliers(dacc_array2, time_array2, std2)\n",
    "count3, ro_array3, ro_time3 = removed_outliers(dacc_array3, time_array3, std3)\n",
    "\n",
    "#print(len(dacc_array1))\n",
    "#print(count1)\n",
    "#print(len(ro_array1))\n",
    "\n",
    "##Set up data interpolation (using Cubic Splines) for use in next step\n",
    "cs1 = CubicSpline(ro_time1, ro_array1)\n",
    "cs2 = CubicSpline(ro_time2, ro_array2)\n",
    "cs3 = CubicSpline(ro_time3, ro_array3)\n",
    "\n",
    "\n",
    "##interpld returns a function that relates y=ro_array (without outliers) to x=time:\n",
    "#cs1 = interp1d(ro_time1, ro_array1)\n",
    "#cs2 = interp1d(ro_time2, ro_array2)\n",
    "#cs3 = interp1d(ro_time3, ro_array3)\n",
    "\n",
    "#Now, use this interpolation to put points back into the original graph:\n",
    "def add_interpolated_pts(a_array, time_array, std, cs):\n",
    "    i = 0\n",
    "    ret_acc = []\n",
    "    while i < (len(a_array)):\n",
    "        if abs(a_array[i]) > std:\n",
    "            ret_acc.append(cs(time_array[i]))\n",
    "        else:\n",
    "            ret_acc.append(a_array[i])\n",
    "        i = i + 1  \n",
    "    return ret_acc;\n",
    "\n",
    "\n",
    "#These are the new arrays with the interpolated points (which we will\n",
    "#feed into a Kalman filter later).\n",
    "interp_array1 = add_interpolated_pts(dacc_array1, time_array1, std1, cs1)\n",
    "interp_array2 = add_interpolated_pts(dacc_array2, time_array2, std2, cs2)\n",
    "interp_array3 = add_interpolated_pts(dacc_array3, time_array3, std3, cs3)\n",
    "\n",
    "\n",
    "#print(len(interp_array1))\n",
    "#print(len(dacc_array1))\n",
    "\n",
    "##To verify that the two arrays are different(i.e. pts were actually interpolated):\n",
    "j = 0\n",
    "count = 0\n",
    "while (j < (len(interp_array1) - 1)):\n",
    "    if interp_array1[j] != dacc_array1[j]:\n",
    "        count = count + 1\n",
    "    j = j + 1\n",
    "        \n",
    "#print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all of the data, to see visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plotting the results for the 1st sub-experiment:\n",
    "f1 = plt.figure(figsize=(10,3))\n",
    "ax1 = f1.add_subplot(121)\n",
    "ax2 = f1.add_subplot(122)\n",
    "\n",
    "f2 = plt.figure(figsize=(10,3))\n",
    "ax3 = f2.add_subplot(121)\n",
    "ax4 = f2.add_subplot(122)\n",
    "\n",
    "ax1.plot(time_array1, acc_array1)\n",
    "ax1.set_title('Acc1 vs. Time')\n",
    "ax1.set_xlabel(\"Time [s]\")\n",
    "ax1.set_ylabel('Acceleration [m/s^2]')\n",
    "#ax1.set_ylim([-6,6])\n",
    "\n",
    "ax2.plot(time_array1, dacc_array1)\n",
    "ax2.set_title(\"Detrended Acc1 vs. Time\")\n",
    "ax2.set_xlabel(\"Time [s]\")\n",
    "ax2.set_ylabel('Acceleration [m/s^2]')\n",
    "ax2.set_ylim([-6,6])\n",
    "\n",
    "ax3.plot(ro_time1, ro_array1)\n",
    "ax3.set_title('Removed Acc1 Outliers vs. Time')\n",
    "ax3.set_xlabel(\"Time [s]\")\n",
    "ax3.set_ylabel('Acceleration [m/s^2]')\n",
    "ax3.set_ylim([-6,6])\n",
    "\n",
    "ax4.plot(time_array1, interp_array1)\n",
    "ax4.set_title(\"Cubic Spline Interpolated Acc1 vs. Time\")\n",
    "ax4.set_xlabel(\"Time [s]\")\n",
    "ax4.set_ylabel('Acceleration [m/s^2]')\n",
    "ax4.set_ylim([-6,6])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plotting the results for the 2nd sub-experiment:\n",
    "f1 = plt.figure(figsize=(10,3))\n",
    "ax1 = f1.add_subplot(121)\n",
    "ax2 = f1.add_subplot(122)\n",
    "\n",
    "f2 = plt.figure(figsize=(10,3))\n",
    "ax3 = f2.add_subplot(121)\n",
    "ax4 = f2.add_subplot(122)\n",
    "\n",
    "\n",
    "##Subplots for acc2\n",
    "ax1.plot(time_array2, acc_array2)\n",
    "ax1.set_title('Acc2 vs. Time')\n",
    "ax1.set_xlabel(\"Time [s]\")\n",
    "ax1.set_ylabel('Acceleration [m/s^2]')\n",
    "ax1.set_ylim([-6,6])\n",
    "ax2.plot(time_array2, dacc_array2)\n",
    "ax2.set_title(\"Detrended Acc2 vs. Time\")\n",
    "ax2.set_xlabel(\"Time [s]\")\n",
    "ax2.set_ylabel('Acceleration [m/s^2]')\n",
    "ax2.set_ylim([-6,6])\n",
    "\n",
    "##Subplots for acc3\n",
    "ax3.plot(ro_time2, ro_array2)\n",
    "ax3.set_title('Removed Acc2 Outliers vs. Time')\n",
    "ax3.set_xlabel(\"Time [s]\")\n",
    "ax3.set_ylabel('Acceleration [m/s^2]')\n",
    "ax3.set_ylim([-6,6])\n",
    "ax4.plot(time_array2, interp_array2)\n",
    "ax4.set_title(\"Cubic Spline Interpolated Acc2 vs. Time\")\n",
    "ax4.set_xlabel(\"Time [s]\")\n",
    "ax4.set_ylabel('Acceleration [m/s^2]')\n",
    "ax4.set_ylim([-6,6])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plotting the results for the 3rd sub-experiment:\n",
    "f1 = plt.figure(figsize=(10,3))\n",
    "ax1 = f1.add_subplot(121)\n",
    "ax2 = f1.add_subplot(122)\n",
    "\n",
    "f2 = plt.figure(figsize=(10,3))\n",
    "ax3 = f2.add_subplot(121)\n",
    "ax4 = f2.add_subplot(122)\n",
    "\n",
    "\n",
    "##Subplots for acc2\n",
    "ax1.plot(time_array3, acc_array3)\n",
    "ax1.set_title('Acc3 vs. Time')\n",
    "ax1.set_xlabel(\"Time [s]\")\n",
    "ax1.set_ylabel('Acceleration [m/s^2]')\n",
    "ax1.set_ylim([-6,6])\n",
    "ax2.plot(time_array3, dacc_array3)\n",
    "ax2.set_title(\"Detrended Acc3 vs. Time\")\n",
    "ax2.set_xlabel(\"Time [s]\")\n",
    "ax2.set_ylabel('Acceleration [m/s^2]')\n",
    "ax2.set_ylim([-6,6])\n",
    "\n",
    "##Subplots for acc3\n",
    "ax3.plot(ro_time3, ro_array3)\n",
    "ax3.set_title('Removed Acc3 Outliers vs. Time')\n",
    "ax3.set_xlabel(\"Time [s]\")\n",
    "ax3.set_ylabel('Acceleration [m/s^2]')\n",
    "ax3.set_ylim([-6,6])\n",
    "ax4.plot(time_array3, interp_array3)\n",
    "ax4.set_title(\"Cubic Spline Interpolated Acc3 vs. Time\")\n",
    "ax4.set_xlabel(\"Time [s]\")\n",
    "ax4.set_ylabel('Acceleration [m/s^2]')\n",
    "ax4.set_ylim([-6,6])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now, feed interpolated arrays through a Kalman filter:\n",
    "##(Actually I'm going to use a LOWESS filter for simplicity)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Here, I'm implementing a lowess filter instead of a Kalman \\n filter right now because it seems simpler to implement.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "lowess = sm.nonparametric.lowess\n",
    "\n",
    "#Parameters: takes in (y,x, ...)\n",
    "filtered1 = lowess(interp_array1, time_array1, frac=0.005, is_sorted=True, return_sorted=False) \n",
    "filtered2 = lowess(interp_array2, time_array2, frac=0.002, is_sorted=True, return_sorted=False) \n",
    "filtered3 = lowess(interp_array3, time_array3, frac=0.002, is_sorted=True, return_sorted=False) \n",
    "\n",
    "\n",
    "##Plotting the filtered results:\n",
    "f1 = plt.figure(figsize=(10,3))\n",
    "ax1 = f1.add_subplot(121)\n",
    "ax2 = f1.add_subplot(122)\n",
    "\n",
    "f2 = plt.figure(figsize=(10,3))\n",
    "ax3 = f2.add_subplot(121)\n",
    "ax4 = f2.add_subplot(122)\n",
    "\n",
    "f3 = plt.figure(figsize=(10,3))\n",
    "ax5 = f3.add_subplot(121)\n",
    "ax6 = f3.add_subplot(122)\n",
    "\n",
    "\n",
    "##Subplots for acc1\n",
    "ax1.plot(time_array1, interp_array1)\n",
    "ax1.set_title('Acc1 vs. Time')\n",
    "ax1.set_xlabel(\"Time [s]\")\n",
    "ax1.set_ylabel('Acceleration [m/s^2]')\n",
    "ax2.plot(time_array1, filtered1)\n",
    "ax2.set_title(\"Lowess Filtered Acc1 vs. Time\")\n",
    "ax2.set_xlabel(\"Time [s]\")\n",
    "ax2.set_ylabel('Acceleration [m/s^2]')\n",
    "\n",
    "##Subplots for acc2\n",
    "ax3.plot(time_array2, interp_array2)\n",
    "ax3.set_title('Acc2 vs. Time')\n",
    "ax3.set_xlabel(\"Time [s]\")\n",
    "ax3.set_ylabel('Acceleration [m/s^2]')\n",
    "ax4.plot(time_array2, filtered2)\n",
    "ax4.set_title(\"Lowess Filtered Acc2 vs. Time\")\n",
    "ax4.set_xlabel(\"Time [s]\")\n",
    "ax4.set_ylabel('Acceleration [m/s^2]')\n",
    "\n",
    "##Subplots for acc3\n",
    "ax5.plot(time_array3, interp_array3)\n",
    "ax5.set_title('Acc3 vs. Time')\n",
    "ax5.set_xlabel(\"Time [s]\")\n",
    "ax5.set_ylabel('Acceleration [m/s^2]')\n",
    "ax6.plot(time_array3, filtered3)\n",
    "ax6.set_title(\"Lowess Filtered Acc3 vs. Time\")\n",
    "ax6.set_xlabel(\"Time [s]\")\n",
    "ax6.set_ylabel('Acceleration [m/s^2]')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Comments: Here, I'm not sure if the Lowess filtered version of Acc3 is better \\n than the original.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double integrate to get Displacement from Acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, find peaks and valleys of the waveforms:\n",
    "#(Found that this works better when the data has been detrended.)\n",
    "\n",
    "indexes0 = peakutils.indexes(acc_array, thres=0.02/max(acc_array), min_dist=100)\n",
    "indexes1 = peakutils.indexes(dacc_array1, thres=0.02/max(dacc_array1), min_dist=100)\n",
    "indexes2 = peakutils.indexes(dacc_array2, thres=0.02/max(dacc_array2), min_dist=100)\n",
    "indexes3 = peakutils.indexes(dacc_array3, thres=0.02/max(dacc_array3), min_dist=100)\n",
    "\n",
    "col_0t = time_array # First column data\n",
    "col_0a = acc_array # Second column data\n",
    "\n",
    "col_1t = time_array1 # First column data\n",
    "col_1a = dacc_array1 # Second column data\n",
    "\n",
    "col_2t = time_array2 # First column data\n",
    "col_2a = dacc_array2 # Second column data\n",
    "\n",
    "col_3t = time_array3 # First column data\n",
    "col_3a = dacc_array3 # Second column data\n",
    "\n",
    "\n",
    "#Index1 gets the peaks, while index2 gets the valleys\n",
    "index_max0 = peakutils.indexes(col_0a, thres=0.66, min_dist=25)\n",
    "index_min0 = peakutils.indexes(-col_0a, thres=0.66, min_dist=25)\n",
    "\n",
    "index_max1 = peakutils.indexes(col_1a, thres=0.66, min_dist=25)\n",
    "index_min1 = peakutils.indexes(-col_1a, thres=0.66, min_dist=25)\n",
    "\n",
    "index_max2 = peakutils.indexes(col_2a, thres=0.66, min_dist=25)\n",
    "index_min2 = peakutils.indexes(-col_2a, thres=0.66, min_dist=25)\n",
    "\n",
    "index_max3 = peakutils.indexes(col_3a, thres=0.66, min_dist=25)\n",
    "index_min3 = peakutils.indexes(-col_3a, thres=0.66, min_dist=25)\n",
    "\n",
    "\n",
    "##Plotting:\n",
    "f1 = plt.figure(figsize=(12,5))\n",
    "ax1 = f1.add_subplot(121)\n",
    "ax2 = f1.add_subplot(122)\n",
    "\n",
    "f2 = plt.figure(figsize=(12,5))\n",
    "ax3 = f2.add_subplot(121)\n",
    "ax4 = f2.add_subplot(122)\n",
    "\n",
    "\n",
    "ax1.plot(col_0t,col_0a, lw=0.4, alpha=0.2, color=\"black\" )\n",
    "ax1.plot(col_0t[index_max0],col_0a[index_max0], marker=\"o\", ls=\"\", ms=3, color=\"red\" )\n",
    "ax1.plot(col_0t[index_min0],col_0a[index_min0], color =\"orange\", marker=\"o\", ls=\"\", ms=3 )\n",
    "ax1.set_title(\"Acceleration vs. Time\")\n",
    "ax1.set_xlabel(\"Time [s]\")\n",
    "ax1.set_ylabel('Acceleration [m/s^2]')\n",
    "ax1.set_xlim([300,850])\n",
    "ax1.set_ylim([-6,6])\n",
    "\n",
    "ax2.plot(col_1t,col_1a, lw=0.4, alpha=0.2, color=\"black\" )\n",
    "ax2.plot(col_1t[index_max1],col_1a[index_max1], marker=\"o\", ls=\"\", ms=3,  color=\"red\" )\n",
    "ax2.plot(col_1t[index_min1],col_1a[index_min1], color =\"orange\", marker=\"o\", ls=\"\", ms=3 )\n",
    "ax2.set_title(\"Acceleration1 vs. Time\")\n",
    "ax2.set_xlabel(\"Time [s]\")\n",
    "ax2.set_ylabel(\"Acceleration [m/s^2]\")\n",
    "#ax2.set_ylim([-6,6])\n",
    "\n",
    "ax3.plot(col_2t,col_2a, lw=0.4, alpha=0.2, color=\"black\" )\n",
    "ax3.plot(col_2t[index_max2],col_2a[index_max2], marker=\"o\", ls=\"\", ms=3,  color=\"red\" )\n",
    "ax3.plot(col_2t[index_min2],col_2a[index_min2], color =\"orange\", marker=\"o\", ls=\"\", ms=3 )\n",
    "ax3.set_title(\"Acceleration2 vs. Time\")\n",
    "ax3.set_xlabel(\"Time [s]\")\n",
    "ax3.set_ylabel(\"Acceleration [m/s^2]\")\n",
    "#ax3.set_ylim([-6,6])\n",
    "\n",
    "ax4.plot(col_3t,col_3a, lw=0.4, alpha=0.2, color=\"black\" )\n",
    "ax4.plot(col_3t[index_max3],col_3a[index_max3], marker=\"o\", ls=\"\", ms=3,  color=\"red\" )\n",
    "ax4.plot(col_3t[index_min3],col_3a[index_min3], color =\"orange\", marker=\"o\", ls=\"\", ms=3 )\n",
    "ax4.set_title(\"Acceleration3 vs. Time\")\n",
    "ax4.set_xlabel(\"Time [s]\")\n",
    "ax4.set_ylabel(\"Acceleration [m/s^2]\")\n",
    "#ax4.set_ylim([-6,6])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerically integrate via peak-picking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(index_max3))\n",
    "#print(len(index_min3))\n",
    "\n",
    "from scipy import integrate\n",
    "x = time_array1\n",
    "y = dacc_array1\n",
    "y_int = integrate.cumtrapz(y, x, initial=0)\n",
    "plt.plot(x, y_int)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD Step 3: FFT of filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, need to fit the data so that the timestamps are *exactly* uniform\n",
    "\n",
    "##PSD of filtered data\n",
    "#psd1 = scipy.signal.periodogram(filtered1)\n",
    "#psd2 = scipy.signal.periodogram(filtered2)\n",
    "#psd3 = scipy.signal.periodogram(filtered3)\n",
    "\n",
    "#These are just in case we have diff. rates later\n",
    "#sampling_rate = 30\n",
    "#inverse_sampling_rate = 1./sampling_rate\n",
    "#timestep = inverse_sampling_rate\n",
    "\n",
    "#For the 30Hz data we will just use timestep = 0.033\n",
    "timestep = 0.033\n",
    "\n",
    "fft1 = abs(np.fft.rfft(filtered1))\n",
    "fft2 = abs(np.fft.rfft(filtered2))\n",
    "fft3 = abs(np.fft.rfft(filtered3))\n",
    "\n",
    "n1 = filtered1.size\n",
    "n2 = filtered2.size\n",
    "n3 = filtered3.size\n",
    "\n",
    "fft_freq1 = np.fft.rfftfreq(n1, timestep)\n",
    "fft_freq2 = np.fft.rfftfreq(n2, timestep)\n",
    "fft_freq3 = np.fft.rfftfreq(n3, timestep)\n",
    "\n",
    "#Here, we want to apply a low pass filter, and we only care about lower frequency signals (smaller than a\n",
    "#frequency of 0.5 or 0.6)\n",
    "\n",
    "#for x,y in zip(filtered1,fft1):\n",
    "#    print(x)\n",
    "#    print(y)\n",
    "\n",
    "#for x in fft1:\n",
    "#    print(x)\n",
    "\n",
    "print(\"Experiment Explanations:\")\n",
    "print(\"Experiment1: We created 'waves' with amplitude of 1m and period of 6s.\")\n",
    "print(\"We expect to see a peak at: 1/6s = 0.16 Hz.\")\n",
    "print(\"Experiment2: We created 'waves' with amplitude 1.5m and period = 4s.\")\n",
    "print(\"We expect to see a peak at: 1/4s = 0.25 Hz.\")\n",
    "print(\"Experiment3: We created 'waves' with amplitude of 1m and period of 2s.\")\n",
    "print(\"We expect to see a peak at: 1/2s = 0.5. Hz.\")\n",
    "\n",
    "##Calculating PSD graph:\n",
    "#Can either estimate this by squaring the FFT or by using the python Periodogram function.\n",
    "\n",
    "freq1, pxx1 = acc_psd1 = signal.periodogram(filtered1, fs=30, scaling='density')\n",
    "freq2, pxx2 = acc_psd2 = signal.periodogram(filtered2, fs=30, scaling='density')\n",
    "freq3, pxx3 = acc_psd3 = signal.periodogram(filtered3, fs=30, scaling='density')\n",
    "\n",
    "\n",
    "#Calculate the peaks:\n",
    "peak_fft1 = fft_freq1[np.argmax(fft1)]\n",
    "peak_fft2 = fft_freq2[np.argmax(fft2)]\n",
    "peak_fft3 = fft_freq3[np.argmax(fft3)]\n",
    "\n",
    "peak_psd1 = freq1[np.argmax(pxx1)]\n",
    "peak_psd2 = freq2[np.argmax(pxx2)]\n",
    "peak_psd3 = freq3[np.argmax(pxx3)]\n",
    "\n",
    "\n",
    "##Plotting the psd results:\n",
    "f1 = plt.figure(figsize=(15,4))\n",
    "ax1 = f1.add_subplot(131)\n",
    "ax2 = f1.add_subplot(132)\n",
    "ax7 = f1.add_subplot(133)\n",
    "\n",
    "f2 = plt.figure(figsize=(15,4))\n",
    "ax3 = f2.add_subplot(131)\n",
    "ax4 = f2.add_subplot(132)\n",
    "ax8 = f2.add_subplot(133)\n",
    "\n",
    "f3 = plt.figure(figsize=(15,4))\n",
    "ax5 = f3.add_subplot(131)\n",
    "ax6 = f3.add_subplot(132)\n",
    "ax9 = f3.add_subplot(133)\n",
    "\n",
    "##Subplots for acc1\n",
    "ax1.plot(time_array1, filtered1)\n",
    "ax1.set_title('Filtered Acc1 vs. Time')\n",
    "ax1.set_xlabel(\"Time [s]\")\n",
    "ax1.set_ylabel('Acceleration [m/s^2]')\n",
    "#------------------------------\n",
    "ax2.plot(fft_freq1, fft1)\n",
    "ax2.set_title(\"FFT Acc1 Expected Frequency\")\n",
    "ax2.set_xlabel(\"Frequency [Hz]\")\n",
    "ax2.set_ylabel('FFT of Acceleration [m/s^2]')\n",
    "ax2.set_xlim(0,0.6)\n",
    "ax2.set_ylim(0,)\n",
    "ax2.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax2.xaxis.set_minor_locator(plt.MultipleLocator(0.05))\n",
    "ax2.axvline(0.16, color=\"orange\")\n",
    "#ax2.axvline(peak_fft1, color=\"red\")\n",
    "#------------------------------\n",
    "ax7.plot(freq1, pxx1)\n",
    "ax7.set_title(\"PSD Acc1 Expected Frequency\")\n",
    "ax7.set_xlabel(\"Frequency [Hz]\")\n",
    "ax7.set_ylabel('PSD of Acceleration [(m/s^2)^2/Hz]')\n",
    "ax7.set_xlim(0,0.6)\n",
    "ax7.set_ylim(0,)\n",
    "ax7.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax7.xaxis.set_minor_locator(plt.MultipleLocator(0.05))\n",
    "ax7.axvline(0.16, color=\"orange\")\n",
    "#ax7.axvline(peak_psd1, color=\"red\")\n",
    "\n",
    "\n",
    "##Subplots for acc2\n",
    "ax3.plot(time_array2, filtered2)\n",
    "ax3.set_title('Filtered Acc2 vs. Time')\n",
    "ax3.set_xlabel(\"Time [s]\")\n",
    "ax3.set_ylabel('Acceleration [m/s^2]')\n",
    "ax4.plot(fft_freq2, fft2)\n",
    "ax4.set_title(\"FFT Acc2 Expected Frequency\")\n",
    "ax4.set_xlabel(\"Frequency [Hz]\")\n",
    "ax4.set_ylabel('FFT of Acceleration [m/s^2]')\n",
    "ax4.set_xlim(0,0.6)\n",
    "ax4.set_ylim(0,)\n",
    "ax4.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax4.xaxis.set_minor_locator(plt.MultipleLocator(0.05))\n",
    "ax4.axvline(0.25, color=\"orange\")\n",
    "ax8.plot(freq2, pxx2)\n",
    "ax8.set_title(\"PSD Acc2 Expected Frequency\")\n",
    "ax8.set_xlabel(\"Frequency [Hz]\")\n",
    "ax8.set_ylabel('PSD of Acceleration [(m/s^2)^2/Hz]')\n",
    "ax8.set_xlim(0,0.6)\n",
    "ax8.set_ylim(0,)\n",
    "ax8.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax8.xaxis.set_minor_locator(plt.MultipleLocator(0.05))\n",
    "ax8.axvline(0.25, color=\"orange\")\n",
    "\n",
    "##Subplots for acc3\n",
    "ax5.plot(time_array3, filtered3)\n",
    "ax5.set_title('Filtered Acc3 vs. Time')\n",
    "ax5.set_xlabel(\"Time [s]\")\n",
    "ax5.set_ylabel('Acceleration [m/s^2]')\n",
    "ax6.plot(fft_freq3, fft3)\n",
    "ax6.set_title(\"FFT Acc3 Expected Frequency\")\n",
    "ax6.set_xlabel(\"Frequency [Hz]\")\n",
    "ax6.set_ylabel('FFT of Acceleration [m/s^2]')\n",
    "ax6.set_xlim(0,0.6)\n",
    "ax6.set_ylim(0,)\n",
    "ax6.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax6.xaxis.set_minor_locator(plt.MultipleLocator(0.05))\n",
    "ax6.axvline(0.5, color=\"orange\")\n",
    "ax9.plot(freq3, pxx3)\n",
    "ax9.set_title(\"PSD Acc3 Expected Frequency\")\n",
    "ax9.set_xlabel(\"Frequency [Hz]\")\n",
    "ax9.set_ylabel('PSD of Acceleration [(m/s^2)^2/Hz]')\n",
    "ax9.set_xlim(0,0.6)\n",
    "ax9.set_ylim(0,)\n",
    "ax9.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax9.xaxis.set_minor_locator(plt.MultipleLocator(0.05))\n",
    "ax9.axvline(0.5, color=\"orange\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#KaiserBessel window (based on the modified zero-order Bessel function) to reduce\n",
    "#spectral leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next steps: Noise-correct acceleration spectra\n",
    "#1. Kaiser Bessel Window before FFT, and then post-processing of data\n",
    "#2. Frequency Domain Filter to acceleration spectra to remove low-frequency noise\n",
    "#   (modification of empirical noise correction of Lang 1987)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displacement Spectra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Noise-corrected Acceleration spectra converted to displacement\n",
    "##spectra by dividing by the frequency to the fourth power.\n",
    "\n",
    "##Check to see if they're the same length:\n",
    "#print(len(fft3))\n",
    "#print(len(fft_freq3))\n",
    "\n",
    "#for x in fft_freq3:\n",
    "#    print(x**4)\n",
    "    \n",
    "##First attempt:   \n",
    "##Iterate over the fft and freq lists to calculate new displacement spectra values.\n",
    "samp_freq = 30\n",
    "\n",
    "def calc_disp(fft, freq):\n",
    "    fft = fft[1:] #Get rid of the first value, since fft_freq's first value is 0, and cannot divide by zero.\n",
    "    freq = freq[1:]\n",
    "    disp_spectra = []\n",
    "    for x_fft, y_freq in zip(fft, freq):\n",
    "        #\"Normalizing\" the PSD?\n",
    "        new_d = float(x_fft**2/(samp_freq**4)) #Need to divide by 30**4 bc sampling freq?\n",
    "        #new_d = float(x_fft/(y_freq**4)) #Using y_freq doesn't work\n",
    "        #print(new_d)\n",
    "        #print(y_freq)\n",
    "        disp_spectra.append(new_d)\n",
    "    return disp_spectra, freq\n",
    "\n",
    "disp_spectra1, fft_nfreq1 = calc_disp(fft1, fft_freq1)\n",
    "disp_spectra2, fft_nfreq2 = calc_disp(fft2, fft_freq2)\n",
    "disp_spectra3, fft_nfreq3 = calc_disp(fft3, fft_freq3)\n",
    "\n",
    "\n",
    "\n",
    "##Second attempt:\n",
    "\n",
    "def calc_disp2(fft, freq):\n",
    "    fft = fft[1:] #Get rid of the first value, since fft_freq's first value is 0, and cannot divide by zero.\n",
    "    freq = freq[1:]\n",
    "    disp_spectra = []\n",
    "    for x_fft, y_freq in zip(fft, freq):\n",
    "        omega = 2*math.pi*y_freq\n",
    "        #new_d = float(x_fft/omega**2) #for FFT?\n",
    "        new_d = float(x_fft/omega**4) #for PSD?\n",
    "        disp_spectra.append(new_d)\n",
    "    return disp_spectra, freq\n",
    "\n",
    "#disp_spectra1, fft_nfreq1 = calc_disp2(fft1, fft_freq1)\n",
    "#disp_spectra2, fft_nfreq2 = calc_disp2(fft2, fft_freq2)\n",
    "#disp_spectra3, fft_nfreq3 = calc_disp2(fft3, fft_freq3)\n",
    "\n",
    "##Attempt using PSD instead of FFT:\n",
    "#disp_spectra1, fft_nfreq1 = calc_disp2(pxx1, freq1)\n",
    "#disp_spectra2, fft_nfreq2 = calc_disp2(pxx2, freq2)\n",
    "#disp_spectra3, fft_nfreq3 = calc_disp2(pxx3, freq3)\n",
    "\n",
    "\n",
    "\n",
    "#for x in disp_spectra1:\n",
    "#    print(x)\n",
    "    \n",
    "    \n",
    "#for x,y in zip(disp_spectra1, fft_nfreq1):\n",
    "#    print(x,y)\n",
    "\n",
    "\n",
    "#disp_spectra1 = np.array(disp_spectra1)\n",
    "\n",
    "##Plotting the psd results:\n",
    "f1 = plt.figure(figsize=(10,3))\n",
    "ax1 = f1.add_subplot(121)\n",
    "ax2 = f1.add_subplot(122)\n",
    "\n",
    "f2 = plt.figure(figsize=(10,3))\n",
    "ax3 = f2.add_subplot(121)\n",
    "ax4 = f2.add_subplot(122)\n",
    "\n",
    "f3 = plt.figure(figsize=(10,3))\n",
    "ax5 = f3.add_subplot(121)\n",
    "ax6 = f3.add_subplot(122)\n",
    "\n",
    "\n",
    "##Subplots for acc1\n",
    "ax1.plot(fft_freq1, fft1)\n",
    "ax1.set_title(\"FFT Acc1 vs. Frequency\")\n",
    "ax1.set_xlabel(\"Frequency [Hz]\")\n",
    "ax1.set_ylabel('FFT of Acceleration [m/s^2]')\n",
    "ax1.set_xlim(0,0.6)\n",
    "ax1.set_ylim(0,)\n",
    "ax1.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax1.xaxis.set_minor_locator(plt.MultipleLocator(0.05))\n",
    "\n",
    "ax2.plot(fft_nfreq1, disp_spectra1)\n",
    "ax2.set_title('FFT Disp1 vs. Frequency')\n",
    "ax2.set_xlabel(\"Frequency [Hz]\")\n",
    "ax2.set_ylabel('Energy Density [m^2/Hz]')\n",
    "ax2.set_xlim(0,0.6)\n",
    "ax2.set_ylim(0,)\n",
    "\n",
    "\n",
    "##Subplots for acc2\n",
    "ax3.plot(fft_freq2, fft2)\n",
    "ax3.set_title(\"FFT Acc2 vs. Frequency\")\n",
    "ax3.set_xlabel(\"Frequency [Hz]\")\n",
    "ax3.set_ylabel('FFT of Acceleration [m/s^2]')\n",
    "ax3.set_xlim(0,0.6)\n",
    "ax3.set_ylim(0,)\n",
    "ax3.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax3.xaxis.set_minor_locator(plt.MultipleLocator(0.05))\n",
    "ax4.plot(fft_nfreq2, disp_spectra2)\n",
    "ax4.set_title('FFT Disp2 vs. Frequency')\n",
    "ax4.set_xlabel(\"Frequency [Hz]\")\n",
    "ax4.set_ylabel('Energy Density [m^2/Hz]')\n",
    "ax4.set_xlim(0,0.6)\n",
    "ax4.set_ylim(0,)\n",
    "\n",
    "##Subplots for acc3\n",
    "ax5.plot(fft_freq3, fft3)\n",
    "ax5.set_title(\"FFT Acc3 vs. Frequency\")\n",
    "ax5.set_xlabel(\"Frequency [Hz]\")\n",
    "ax5.set_ylabel('FFT of Acceleration [m/s^2]')\n",
    "ax5.set_xlim(0,0.6)\n",
    "ax5.set_ylim(0,)\n",
    "ax5.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax5.xaxis.set_minor_locator(plt.MultipleLocator(0.05))\n",
    "ax6.plot(fft_nfreq3, disp_spectra3)\n",
    "ax6.set_title('FFT Disp3 vs. Frequency')\n",
    "ax6.set_xlabel(\"Frequency [Hz]\")\n",
    "ax6.set_ylabel('Energy Density [m^2/Hz]')\n",
    "ax6.set_xlim(0,0.6)\n",
    "ax6.set_ylim(0,)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Wave Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the peak wave period: get the x-axis value at the peak y-axis value\n",
    "peak_freq1 = fft_nfreq1[np.argmax(disp_spectra1)]\n",
    "peak_freq2 = fft_nfreq2[np.argmax(disp_spectra2)]\n",
    "peak_freq3 = fft_nfreq3[np.argmax(disp_spectra3)]\n",
    "\n",
    "print(\"Calculating Peak Frequency:\")\n",
    "print(\"Expected near 0.16 Hz and got: \", peak_freq1, \"Hz\")\n",
    "print(\"Expected near 0.25 Hz and got: \", peak_freq2, \"Hz\")\n",
    "print(\"Expected near 0.5 Hz and got: \", peak_freq3, \"Hz\")\n",
    "\n",
    "\n",
    "#Print out the peak wave frequency, which is 1/peak_period\n",
    "peak_period1 = 1/peak_freq1\n",
    "peak_period2 = 1/peak_freq2\n",
    "peak_period3 = 1/peak_freq3\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Calculating Peak Period:\")\n",
    "print(\"Expected near 6 secs and got: \", peak_period1, \"secs\")\n",
    "print(\"Expected near 4 secs  and got: \", peak_period2, \"secs\")\n",
    "print(\"Expected near 2 secs and got: \", peak_period3, \"secs\")\n",
    "\n",
    "##Calculate the mean variance of the sea surface elevation, by taking the integral under the FFT curve\n",
    "#We only want to integrate while fft_nfreq < 0.6:\n",
    "\n",
    "def limit_frequency_range(fft_freq):\n",
    "    ret_freq = []\n",
    "    for x in fft_freq:\n",
    "        if x < 0.6:\n",
    "            ret_freq.append(x)\n",
    "    return ret_freq\n",
    "\n",
    "lim_freq1 = limit_frequency_range(fft_nfreq1)\n",
    "lim_freq2 = limit_frequency_range(fft_nfreq2)\n",
    "lim_freq3 = limit_frequency_range(fft_nfreq3)\n",
    "\n",
    "#Need to make disp_spectra match length of newly limited frequency:\n",
    "len1 = len(lim_freq1)\n",
    "len2 = len(lim_freq2)\n",
    "len3 = len(lim_freq3)\n",
    "\n",
    "lim_dspectra1 = disp_spectra1[0:len1]\n",
    "lim_dspectra2 = disp_spectra2[0:len2]\n",
    "lim_dspectra3 = disp_spectra3[0:len3]\n",
    "\n",
    "m0_1 = simps(y=lim_dspectra1, x=lim_freq1)\n",
    "m0_2 = simps(y=lim_dspectra2, x=lim_freq2)\n",
    "m0_3 = simps(y=lim_dspectra3, x=lim_freq3)\n",
    "\n",
    "Hs1 = 4*math.sqrt(m0_1)\n",
    "Hs2 = 4*math.sqrt(m0_2)\n",
    "Hs3 = 4*math.sqrt(m0_3)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Calculating Significant Wave Heights:\")\n",
    "print(\"Expecting near 1m and got: \", Hs1, \"[m]\")\n",
    "print(\"Expecting near 1.5m and got: \", Hs2, \"[m]\")\n",
    "print(\"Expecting near 1m and got: \", Hs3, \"[m]\")\n",
    "\n",
    "\n",
    "#Calculate the significant wave height by taking 4*sqrt(mean_variance^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
